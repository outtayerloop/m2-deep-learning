{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Real dataset\n",
        "\n",
        "So far we have used only dataset which were downloaded. \n",
        "But it is useful to know how to get a real dataset and load it with keras.\n",
        "\n",
        "\n",
        "In this notebook you will have to train a keras model on a small dataset you collected yourself ! \n",
        "\n",
        "Option 1 : you collect 10 examples of each class for the problem of your choice \n",
        "\n",
        "Option 2 : you teamup and each member of the team upload to a google drive 10 examples per classes. \n",
        "\n",
        "\n",
        "Create one folder per class. \n",
        "\n",
        "Exemple \n",
        "\n",
        "if you want to classify cats and dogs you must have a folder with dogs pictures and a folder with cat pictures\n",
        "\n",
        "like \n",
        "\n",
        "dog/\n",
        "\n",
        "cat/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00001-c83cfa17-2868-4de6-b29d-627a2f969526",
        "id": "KVBYmi9girNH",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the **image_dataset_from_directory** function of keras, load your dataset into a variable named **train**\n",
        "\n",
        "use the following parameters\n",
        "- labels='inferred'\n",
        "- label_mode='categorical'\n",
        "- image_size=(64,64)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00002-5b5a9048-4b00-4dd8-aaa8-61b6e61727ee",
        "id": "LzWJiZTXirNO",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00002-71c5e59f-1051-4d0f-968c-5a2f3cd32876",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a2cd9376",
        "execution_start": 1639657663669,
        "execution_millis": 2,
        "deepnote_cell_type": "code",
        "id": "TgmGWjqcTe9a"
      },
      "source": [
        "import tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00002-bcd6adae-dd6b-4789-a93c-1f132acf78ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8Ciuc9_irNP",
        "outputId": "10b2b177-db72-45c8-bb56-afddb94c1e19",
        "deepnote_output_heights": [
          null,
          21.1875
        ],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "f8890aac",
        "execution_start": 1639657665795,
        "execution_millis": 473,
        "deepnote_cell_type": "code"
      },
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "# label_mode='categorical' one-hot-encodes the labels (no sparse_categorical_entropy loss in the model)\n",
        "train = image_dataset_from_directory('classification', label_mode='categorical', labels='inferred', image_size=(64,64))\n",
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Found 20 files belonging to 2 classes.\n",
          "output_type": "stream",
          "data": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "<BatchDataset shapes: ((None, 64, 64, 3), (None, 2)), types: (tf.float32, tf.float32)>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00004-4242a320-f9b0-45b7-b8fe-8f749db8c8a4",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "19e9bc7c",
        "execution_start": 1639657669238,
        "execution_millis": 58,
        "deepnote_output_heights": [
          21.1875
        ],
        "deepnote_cell_type": "code",
        "id": "sM3IZj93Te9e",
        "outputId": "37578a47-e78a-4f50-ffc6-89f1d47a85c0"
      },
      "source": [
        "X_train, y_train = next(iter(train))\n",
        "X_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "(TensorShape([20, 64, 64, 3]), TensorShape([20, 2]))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loop over the dataset and display the shape of the iteration value"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00002-9d0138cf-520f-40ff-a4f1-ccad7171e1d9",
        "id": "hEAwjGB5irNR",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj6bTu32kVKQ",
        "outputId": "67682346-a9db-4361-f215-8c6217ebfbfc",
        "cell_id": "00007-d8726757-bd10-48e6-8fed-a40e9511caa1",
        "deepnote_output_heights": [
          21.1875
        ],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "9277e112",
        "execution_start": 1639657672795,
        "execution_millis": 9,
        "deepnote_cell_type": "code"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class_values = np.unique(y_train)\n",
        "class_count = len(class_values)\n",
        "class_count, class_values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "(2, array([0., 1.], dtype=float32))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWr0xPEOlFSH",
        "outputId": "f742b5d6-ca38-41ac-86bb-bbadc7751798",
        "cell_id": "00008-86bc4ec2-26fb-4780-9b13-6ef37372e3dc",
        "deepnote_output_heights": [
          21.1875
        ],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a2d66ba6",
        "execution_start": 1639657675975,
        "execution_millis": 5,
        "deepnote_cell_type": "code"
      },
      "source": [
        "image_height = X_train.shape[1]\n",
        "image_width = X_train.shape[2]\n",
        "color_count = X_train.shape[3]\n",
        "image_height, image_width, color_count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "(64, 64, 3)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a small convolutional model and train it on the dataset "
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00003-90ae1b16-1bef-4960-8949-fdb91b1c318c",
        "id": "El6giytOirNT",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00006-1632998d-d462-42ad-b7aa-86bbf9370880",
        "id": "6dkcwJuTirNU",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "15f2e1e8",
        "execution_start": 1639657681391,
        "execution_millis": 1,
        "deepnote_cell_type": "code"
      },
      "source": [
        "X_train = X_train / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClfkeTOmk4Xl",
        "cell_id": "00011-91c5fe7b-1705-411c-8a7e-0214909fda0b",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "dedfcc1b",
        "execution_start": 1639657710174,
        "execution_millis": 2,
        "deepnote_cell_type": "code"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRwdsRG-lA0-",
        "cell_id": "00012-7a310a08-0134-4f35-b3d2-8ead95c920ca",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "422595c6",
        "execution_start": 1639657726568,
        "execution_millis": 476,
        "deepnote_cell_type": "code"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, activation='relu', kernel_size=[3,3], input_shape=[image_height, image_width, color_count]))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=300, activation='relu'))\n",
        "model.add(Dense(units=class_count, activation='softmax')) # as many neurones as classes\n",
        "# softmax normalizes the model's outputs so that it looks like a proba distribution with Sum(output_i) = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6-xOiSaldv5",
        "cell_id": "00013-18d818cb-79b5-436c-aa86-f35e4186201c",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a6e012f4",
        "execution_start": 1639657733241,
        "execution_millis": 6,
        "deepnote_cell_type": "code"
      },
      "source": [
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "# categorical_crossentropy : used when class values are already one-hot-encoded\n",
        "# sparse_categorical_crossentropy : used when class values are not already one-hot-encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00013-1ab2f060-6f59-4793-a7eb-cd76c21af0f7",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "9f51fb0",
        "execution_start": 1639657736120,
        "execution_millis": 54,
        "deepnote_output_heights": [
          21.1875
        ],
        "deepnote_cell_type": "code",
        "id": "asAZqkp4Te9l",
        "outputId": "3de0dff3-e5c6-48eb-da6f-04d2e0ec09e5"
      },
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "(TensorShape([20, 64, 64, 3]), TensorShape([20, 2]))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmVrIDUilgLK",
        "cell_id": "00014-4c767d9b-b3dd-44db-a696-3999bddba2c0",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "c6a6a396",
        "execution_start": 1639657754435,
        "execution_millis": 5196,
        "deepnote_output_heights": [
          null,
          21.1875
        ],
        "deepnote_cell_type": "code",
        "outputId": "e75db761-8812-4065-fae2-e4c3455b3c9e"
      },
      "source": [
        "model_history = model.fit(X_train, y_train, validation_split=0.3, epochs=10)\n",
        "model_history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/10\n1/1 [==============================] - 1s 1s/step - loss: 0.7163 - accuracy: 0.3571 - val_loss: 1.1370 - val_accuracy: 0.5000\nEpoch 2/10\n1/1 [==============================] - 1s 526ms/step - loss: 1.2754 - accuracy: 0.5000 - val_loss: 6.0432 - val_accuracy: 0.5000\nEpoch 3/10\n1/1 [==============================] - 0s 452ms/step - loss: 7.5054 - accuracy: 0.5000 - val_loss: 0.6865 - val_accuracy: 0.5000\nEpoch 4/10\n1/1 [==============================] - 0s 358ms/step - loss: 0.6815 - accuracy: 0.7857 - val_loss: 0.6865 - val_accuracy: 0.6667\nEpoch 5/10\n1/1 [==============================] - 0s 449ms/step - loss: 0.6720 - accuracy: 0.9286 - val_loss: 0.6868 - val_accuracy: 0.6667\nEpoch 6/10\n1/1 [==============================] - 0s 422ms/step - loss: 0.6637 - accuracy: 1.0000 - val_loss: 0.6871 - val_accuracy: 0.6667\nEpoch 7/10\n1/1 [==============================] - 0s 380ms/step - loss: 0.6546 - accuracy: 1.0000 - val_loss: 0.6858 - val_accuracy: 0.6667\nEpoch 8/10\n1/1 [==============================] - 0s 441ms/step - loss: 0.6456 - accuracy: 1.0000 - val_loss: 0.6860 - val_accuracy: 0.5000\nEpoch 9/10\n1/1 [==============================] - 0s 404ms/step - loss: 0.6364 - accuracy: 0.9286 - val_loss: 0.6844 - val_accuracy: 0.5000\nEpoch 10/10\n1/1 [==============================] - 0s 462ms/step - loss: 0.6267 - accuracy: 1.0000 - val_loss: 0.6833 - val_accuracy: 0.5000\n",
          "output_type": "stream",
          "data": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f42641bee50>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data augmentation \n",
        "\n",
        "We now want to do Data augmentation. \n",
        "\n",
        "Data augmentation is a technique to artificially increase the dataset. \n",
        "\n",
        "Image data augmentation idea is pretty simple : you will apply transformation on each image to generate additional synthetic example. You can for example do  rotation, cropping, luminosity changes, zooming, etc. \n",
        "\n",
        "Keras offer several data-augmentation techniques. \n",
        "\n",
        "For the image they are here : https://keras.io/api/layers/preprocessing_layers/image_augmentation/\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00004-c00b58cb-9c4f-4b2b-9224-4570dd25f801",
        "id": "lSksV9PoirNV",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Build a model by adding the layer RandomRotation and RandomTranslation \n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00005-c232b9b0-2422-43e5-b3d0-0ccf01d427e0",
        "id": "i0Ei9EIqirNX",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00017-07318b74-3758-4685-9a63-74a79625a9ca",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "be3a1d88",
        "execution_start": 1639658340912,
        "execution_millis": 0,
        "deepnote_cell_type": "code",
        "id": "ksV9IlE5Te9n"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import RandomRotation, RandomTranslation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00010-13896e4a-ee8a-4ff4-aae4-d59d39cb428d",
        "id": "9cD9SVFSirNY",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "8c79cde8",
        "execution_start": 1639658394868,
        "execution_millis": 577,
        "deepnote_cell_type": "code"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(RandomRotation(factor=(-0.2, 0.3), input_shape=[image_height, image_width, color_count]))\n",
        "model.add(RandomTranslation(height_factor=(-0.2, 0.3), width_factor=(0.2, 0.3)))\n",
        "model.add(Conv2D(32, activation='relu', kernel_size=[3,3], input_shape=[image_height, image_width, color_count]))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=300, activation='relu'))\n",
        "model.add(Dense(units=class_count, activation='softmax')) # as many neurones as classes\n",
        "# softmax normalizes the model's outputs so that it looks like a proba distribution with Sum(output_i) = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "# categorical_crossentropy : used when class values are already one-hot-encoded\n",
        "# sparse_categorical_crossentropy : used when class values are not already one-hot-encoded"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00020-25ca9f2b-7aa9-4954-8c53-455eefce880c",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a6e012f4",
        "execution_start": 1639658398171,
        "execution_millis": 8,
        "deepnote_cell_type": "code",
        "id": "tWkE7v7pTe9n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "train the model again "
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00006-da8a52ca-7c25-47ba-b49d-75cfabcbb580",
        "id": "KhoZdfSYirNZ",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00012-899ff3df-78c5-4c6c-ac3a-ea3f40c75171",
        "id": "rEFOIKgeirNa",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "c6a6a396",
        "execution_start": 1639658400685,
        "execution_millis": 5671,
        "deepnote_output_heights": [
          null,
          21.1875
        ],
        "deepnote_cell_type": "code",
        "outputId": "ea57d38b-89fd-4355-9642-028a6d59fa82"
      },
      "source": [
        "model_history = model.fit(X_train, y_train, validation_split=0.3, epochs=10)\n",
        "model_history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/10\n1/1 [==============================] - 1s 1s/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 2.0425 - val_accuracy: 0.5000\nEpoch 2/10\n1/1 [==============================] - 0s 450ms/step - loss: 2.5170 - accuracy: 0.5000 - val_loss: 2.7495 - val_accuracy: 0.5000\nEpoch 3/10\n1/1 [==============================] - 0s 443ms/step - loss: 3.1880 - accuracy: 0.5000 - val_loss: 0.7114 - val_accuracy: 0.5000\nEpoch 4/10\n1/1 [==============================] - 0s 420ms/step - loss: 0.7325 - accuracy: 0.5000 - val_loss: 0.6892 - val_accuracy: 0.5000\nEpoch 5/10\n1/1 [==============================] - 0s 477ms/step - loss: 0.6822 - accuracy: 0.6429 - val_loss: 0.6873 - val_accuracy: 0.5000\nEpoch 6/10\n1/1 [==============================] - 0s 498ms/step - loss: 0.6819 - accuracy: 0.5714 - val_loss: 0.6824 - val_accuracy: 0.5000\nEpoch 7/10\n1/1 [==============================] - 1s 505ms/step - loss: 0.6895 - accuracy: 0.5714 - val_loss: 0.6770 - val_accuracy: 0.6667\nEpoch 8/10\n1/1 [==============================] - 0s 414ms/step - loss: 0.6822 - accuracy: 0.5714 - val_loss: 0.6728 - val_accuracy: 0.6667\nEpoch 9/10\n1/1 [==============================] - 0s 443ms/step - loss: 0.6770 - accuracy: 0.6429 - val_loss: 0.6724 - val_accuracy: 0.5000\nEpoch 10/10\n1/1 [==============================] - 0s 488ms/step - loss: 0.6834 - accuracy: 0.5000 - val_loss: 0.6647 - val_accuracy: 0.6667\n",
          "output_type": "stream",
          "data": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f42535725d0>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00007-be1ef54a-31a1-4452-a4de-0ba518ef5119",
        "id": "TEduhuPKirNa",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=678e1752-4f32-4619-b3d3-557ab0f005a3' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ],
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "PJEgQQpUTe9p"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_notebook_id": "c9c091f4-4755-4342-a5ad-2de79de3f568",
    "deepnote_execution_queue": [],
    "colab": {
      "name": "4_real_dataset_and_data_augmentation.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  }
}